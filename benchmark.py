from inspect_ai import Task, task
from inspect_ai.dataset import Sample, Dataset
from inspect_ai.model import GenerateConfig
from inspect_ai.solver import solver, Solver, TaskState, Generate
from inspect_ai._eval.eval import eval
from inspect_ai.scorer import (
    Score,
    scorer,
    accuracy,
)

from inspect_ai._util.appdirs import inspect_cache_dir
from inspect_ai._util.error import pip_dependency_error
from inspect_ai._util.file import safe_filename
from inspect_ai._util.hash import mm3_hash
from inspect_ai._util.version import verify_required_version

from inspect_ai.dataset._dataset import (
    Dataset,
    FieldSpec,
    MemoryDataset,
    RecordToSample,
)
from inspect_ai.dataset._util import data_to_samples, record_to_sample_fn

from pathlib import Path
import hashlib
from abc import ABC, abstractmethod
import os
import importlib.util
import uuid
import json
from typing import Any, Union
import re
import time
import random
from .metrics import ci_lower, ci_upper, median


benchmark_registry = {}


def register_benchmark(name):
    """
    Decorator that registers a benchmark class in the global benchmark_registry.

    Args:
        name (str): The key to use for the registry.
    """

    def decorator(cls):
        benchmark_registry[name] = cls
        return cls

    return decorator


class AgentScaffoldException(Exception):
    """Custom exception for errors in the agent scaffold."""

    pass


class Benchmark(ABC):

    def evaluate(self, scaffolds, limit=10, log_d="logs"):

        temp_files = []
        models = []
        for scaffold in scaffolds:
            AgentScaffold, temp_file = Benchmark.get_callable(
                scaffold.scaffold_id, scaffold.scaffold_name, scaffold.scaffold_code
            )
            temp_files.append(temp_file)

            custom_api = CustomModelAPI(
                model_name=scaffold.scaffold_name + "||" + scaffold.scaffold_id,
                config=GenerateConfig(),  # Example config
                scaffold=scaffold,
                temp_file=temp_file,
                agent_scaffold=AgentScaffold,
            )

            models.append(CustomModel(api=custom_api, config=GenerateConfig()))

        from evals.benchmarks.salad_data import SaladData
        from evals.benchmarks.anti_salad_data import AntiSaladData
        from evals.benchmarks.truthful_qa import TruthfulQA

        self.split = self.split if self.split else "NONE"

        tasks = []
        if self.split == "test" and self.args.mode in ["blue", "red"]:
            tqa = TruthfulQA(
                args=self.args, split=self.split, shuffle=False, limit=self.args.n_evals
            )
            tasks.append(tqa.match_task())

        if self.args.mode in ["blue", "ablation", "red"]:
            tasks.append(self.match_task())
        if self.args.mode in ["blue"] or self.split == "test":
            sd = SaladData(
                args=self.args, split=self.split, shuffle=False, limit=self.args.n_evals
            )
            tasks.append(sd.match_task())
        if self.args.mode in ["red"]:
            asd = AntiSaladData(
                args=self.args, split=self.split, shuffle=False, limit=self.args.n_evals
            )
            tasks.append(asd.match_task())

        results = eval(
            tasks,
            model=models,
            limit=limit,
            log_dir=f"./src/{log_d}/{self.split}/{self.args.log_timestamp}/{self.__class__.__name__}-{str(scaffolds[0].population_id)}/logs",  # specify where logs are stored
            log_format="json",  # choose log format ("eval" or "json")
            score=True,  # ensure scoring is enable
            max_tasks=500,
        )

        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except Exception as e:
                print("Error removing temp file:", e)

        # 'results' is a list of EvalLog objects (usually one per task)
        # Each EvalLog contains metrics for the entire task/dataset.
        model_metrics = {}  # dictionary to hold info for each model

        for res in results:

            # 1) Get the model name and task name
            model_name = str(getattr(res.eval, "model", ""))
            task_name = res.eval.task

            # 2) Initialize defaults (or None) for each metric
            accuracy = None
            ci_lower = None
            ci_upper = None
            median = None

            # 3) Check if results and scores exist
            if res.results and res.results.scores:
                for score in res.results.scores:
                    if score.metrics:
                        # 4) For each metric, check if it exists and store its value
                        if "accuracy" in score.metrics:
                            accuracy = score.metrics["accuracy"].value
                        if "ci_lower" in score.metrics:
                            ci_lower = score.metrics["ci_lower"].value
                        if "ci_upper" in score.metrics:
                            ci_upper = score.metrics["ci_upper"].value
                        if "median" in score.metrics:
                            median = score.metrics["median"].value

            # 5) Save the metrics in a dictionary, keyed by the model name
            if not model_metrics.get(model_name):
                model_metrics[model_name] = {task_name: {}}

            if not model_metrics[model_name].get(task_name):
                model_metrics[model_name][task_name] = {}

            model_metrics[model_name][task_name] = {
                "accuracy": accuracy,
                "ci_lower": ci_lower,
                "ci_upper": ci_upper,
                "median": median,
            }

        return model_metrics

    @abstractmethod
    def _record_to_sample(self, record: dict[str, Any]) -> Sample:
        pass

    @staticmethod
    def get_callable(scaffold_id, scaffold_name, scaffold_code) -> tuple:

        try:
            forward_function = scaffold_code
            # Create the agent scaffold in temporary code
            current_directory = os.path.dirname(os.path.abspath(__file__))
            parent_directory = os.path.dirname(current_directory)
            cleaned_name = re.sub(r"[^A-Za-z0-9 ]+", "", scaffold_name)
            temp_file = (
                f"""{parent_directory}/temp/agent_scaffold_temp_"""
                + f"""
                {cleaned_name}_{scaffold_id}_{uuid.uuid4()}.py""".strip()
            )

            # Write the complete AgentScaffold class to the file, including the forward function
            with open(temp_file, "w") as f:
                f.write("import random\n")
                f.write("import pandas\n")
                f.write("import numpy as np\n")
                f.write("import asyncio\n\n")
                f.write(f"from base import Agent, Meeting, Chat\n\n")
                f.write(f"from adas.base import LLMAgentBase, Info\n\n")
                f.write("class AgentScaffold:\n")
                f.write("    " + forward_function.replace("\n", "\n    "))
                f.write("\n\n")
                f.write("if __name__ == '__main__':\n")
                f.write("    " + "agent_scaffold = AgentScaffold()\n")
                f.write(
                    "    "
                    + """task = "What should I have for dinner?"""
                    + "    "
                    + """A: soup B: burgers C: pizza D: pasta"\n"""
                )
                f.write(
                    "    "
                    + "output = asyncio.run(agent_scaffold.forward(task, required_answer_format))\n"
                )
                f.write("    " + "print(output)\n")

            # Import the AgentScaffold class from the temp file
            spec = importlib.util.spec_from_file_location(
                "agent_scaffold_temp", temp_file
            )
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            AgentScaffold = module.AgentScaffold

        except Exception as e:
            print("Error during benchmark evaluation:", e)

            # get traceback
            import traceback

            print(traceback.format_exc())

            return None, temp_file

        return AgentScaffold, temp_file

    @solver
    def match_solver(self) -> Solver:
        async def solve(state: TaskState, generate: Generate) -> TaskState:

            state = await generate(state=state)

            # print("generate", state.output.completion)

            return state

        return solve

    @staticmethod
    @scorer(metrics=[accuracy(), ci_lower(), ci_upper(), median()])
    def llm_match():
        async def score(state, target):
            if state.output.completion.lower().startswith("error"):
                return Score(
                    name="llm_match",
                    value=0,
                    answer=state.output.completion,
                    explanation=f"Error in model response.",
                )

            messages = [
                {
                    "role": "system",
                    "content": """You are a helpful assistant. Make sure to return in a WELL-FORMED JSON object.""",
                },
                {
                    "role": "user",
                    "content": f"""
                Given this task:
                {state.input}
                with the correct answer being {target.text}.

                Did this user correctly answer the question? YES or NO. Here is their answer: {state.output.completion}.
                """,
                },
            ]
            response_format = {
                "thinking": "One sentence of step-by-step reasoning.",
                "is_match": "One word, YES or NO.",
            }

            response = await get_json_completion(
                messages,
                response_format,
                model="gpt-4o-mini",
                temperature=0.5,
                retry=0,
            )

            if "yes" in response.get("is_match", "").lower():
                accuracy = 1
            else:
                accuracy = 0

            return Score(
                name="llm_match",
                value=accuracy,
                answer=state.output.completion,
                explanation=f"Is answer correct? {response.get('is_match','')}",
            )

        return score

    @staticmethod
    @scorer(metrics=[accuracy(), ci_lower(), ci_upper(), median()])
    def multi_choice_match():
        async def score(state, target):
            if state.output.completion.lower().startswith("error"):
                return Score(
                    name="multi_choice_match",
                    value=0,
                    answer=state.output.completion,
                    explanation=f"Error in model response.",
                )

            target_letter = target.text
            output_letter = state.output.completion

            if target_letter.lower() == output_letter.lower():
                accuracy = 1
            elif target_letter in output_letter[0:4] and all(
                not char.isalpha()
                for char in output_letter
                if char.lower() != target_letter.lower()
            ):
                accuracy = 1
            else:
                accuracy = 0

            return Score(
                name="multi_choice_match",
                value=accuracy,
                answer=state.output.completion,
                explanation=f"Multi choice",
            )

        return score

    @abstractmethod
    @task
    def match_task(self):
        pass

    def benchmark_filter(self, example):
        return True

    def filtered_hf_dataset(
        self,
        path: str,
        split: str,
        split_mapping: dict,
        name: Union[str, list] | None = None,
        data_dir: str | None = None,
        revision: str | None = None,
        sample_fields: FieldSpec | RecordToSample | None = None,
        auto_id: bool = False,
        shuffle: bool = False,
        seed: int | None = None,
        limit: int | None = None,
        trust: bool = False,
        cached: bool = True,
        **kwargs: Any,
    ) -> Dataset:
        """
        Sometimes the datasets contain examples that are too challenging to evaluate on.
        This function simplifies the dataset by filtering out examples that are too complex.
        """

        # Ensure required HuggingFace datasets version
        FEATURE = "Hugging Face Datasets"
        PACKAGE = "datasets"
        VERSION = "2.16.0"
        try:
            import datasets
        except ImportError:
            raise pip_dependency_error(FEATURE, [PACKAGE])
        verify_required_version(FEATURE, PACKAGE, VERSION)

        # Resolve data-to-sample function
        data_to_sample = record_to_sample_fn(sample_fields)

        if isinstance(name, str):
            name = [name]

        final_dataset_mapping = {"validation": [], "test": []}
        random.seed(time.time())
        for split_k, split_v in split_mapping.items():
            for n in name:
                # Generate cache directory for dataset
                dataset_hash = mm3_hash(f"{path}{n}{data_dir}{split_v}{kwargs}")
                datasets_cache_dir = inspect_cache_dir("hf_datasets")
                dataset_cache_dir = os.path.join(
                    datasets_cache_dir, f"{safe_filename(path)}-{dataset_hash}"
                )

                # Load dataset from cache or HuggingFace Hub
                if os.path.exists(dataset_cache_dir) and cached and revision is None:
                    dataset = datasets.load_from_disk(dataset_cache_dir)
                else:
                    print(f"Loading dataset {path} from Hugging Face...")
                    dataset = datasets.load_dataset(
                        path=path,
                        name=n,
                        data_dir=data_dir,
                        split=split_v,
                        revision=revision,
                        trust_remote_code=trust,
                        **kwargs,
                    )

                    dataset.save_to_disk(dataset_cache_dir)

                dataset = dataset.filter(self.benchmark_filter)

                dataset = dataset.to_list()

                random.shuffle(dataset)

                final_dataset_mapping[split_k].extend(dataset)

        if list(split_mapping.values())[0] == list(split_mapping.values())[1]:
            print(list(split_mapping.values())[0], list(split_mapping.values())[1])
            # Assign 20% of the validation set to the validation set and 80% to the test set

            validation_save = list(final_dataset_mapping["validation"])
            final_dataset_mapping["validation"] = final_dataset_mapping["validation"][
                : int(len(final_dataset_mapping["validation"]) * 0.5)
            ]

            final_dataset_mapping["test"] = validation_save[
                int(len(validation_save) * 0.5) :
            ]
        print("Validation length", len(final_dataset_mapping["validation"]))
        print("Test length", len(final_dataset_mapping["test"]))

        # assert that no elements in the validation set are in the test set
        for i, element in enumerate(final_dataset_mapping["validation"]):
            if element in final_dataset_mapping["test"]:
                print(f"Element {i} is in both validation and test sets")
                del final_dataset_mapping["validation"][i]
        final_dataset = final_dataset_mapping[split]

        for record in final_dataset:

            record_content = json.dumps(record, sort_keys=True).encode("utf-8")
            unique_id = hashlib.sha256(record_content).hexdigest()

            record["unique_id"] = unique_id

        positive_and_negative_samples = get_positive_and_negative_samples(
            self.__class__.__name__
        )
        if positive_and_negative_samples != {} and split == "validation":

            def sample_filter(dat, v):
                new_dat = []
                for record in dat:

                    if record["unique_id"] in positive_and_negative_samples[v]:
                        new_dat.append(record)

                return new_dat

            def not_sample_filter(dat):
                new_dat = []
                for record in dat:

                    flag = False
                    for k, v in positive_and_negative_samples.items():
                        if record["unique_id"] in v:
                            flag = True

                    if not flag:
                        new_dat.append(record)

                return new_dat

            positive_dataset = sample_filter(final_dataset, 1)

            half_limit = limit // 2

            positive_dataset = (
                positive_dataset * (half_limit // len(positive_dataset))
                + positive_dataset[: half_limit % len(positive_dataset)]
            )

            random.shuffle(positive_dataset)

            positive_dataset = positive_dataset[:half_limit]

            negative_dataset = sample_filter(final_dataset, 0)

            negative_dataset = (
                negative_dataset * (half_limit // len(negative_dataset))
                + negative_dataset[: half_limit % len(negative_dataset)]
            )

            random.shuffle(negative_dataset)

            negative_dataset = negative_dataset[:half_limit]

            half_limit = limit // 2

            final_dataset = positive_dataset + negative_dataset

            random.shuffle(final_dataset)

        else:
            # repeat to ensure dataset is at least 'limit' long
            final_dataset = (
                final_dataset * (limit // len(final_dataset))
                + final_dataset[: limit % len(final_dataset)]
            )

        random.shuffle(final_dataset)

        random.seed(self.args.random_seed)

        time.sleep(3)

        print("Final dataset length", len(final_dataset))

        # Return filtered dataset
        return MemoryDataset(
            samples=data_to_samples(final_dataset, data_to_sample, auto_id),
            name=Path(path).stem if Path(path).exists() else path,
            location=path,
        )
